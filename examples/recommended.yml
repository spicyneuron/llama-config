proxy:
  listen: "localhost:8081"
  target: "http://localhost:8080"

rules:
  - methods: POST
    paths: .*/chat/completions

    # Normalize request path
    target_path: /v1/chat/completions

    on_request:
      - default:
          max_tokens: 10000

      # Apply aliases
      - match_body:
          model: r1
        merge:
          model: deepseek-r1

      # Apply model-specific configurations
      - match_body:
          model: deepseek-?(v3|r1)
        merge:
          temperature: 0.6
          top_p: 0.95

      - match_body:
          model: gemma-?3
        merge:
          temperature: 1
          top_k: 64
          top_p: 0.95

      - match_body:
          model: glm-?4
        merge:
          temperature: 0.6

      - match_body:
          model: gpt-oss
        merge:
          temperature: 1
          top_k: 0

      - match_body:
          model: qwen-?3.*-coder
        merge:
          temperature: 0.7
          top_k: 20
          top_p: 0.8
          repeat_penalty: 1.05

      - match_body:
          model: qwen-?3.*-instruct
        merge:
          temperature: 0.7
          top_k: 20
          top_p: 0.8

      - match_body:
          model: qwen-?3.*-thinking
        merge:
          temperature: 0.6
          top_k: 20
          top_p: 0.95
# LLM Config Proxy

Managing local LLM settings is a pain. Each model's recommended parameters are different, but each client must be configured separately.

This simple, lightweight proxy sits in between your LLM client(s) and server(s),dutifully applying your preferred configuration to each model.